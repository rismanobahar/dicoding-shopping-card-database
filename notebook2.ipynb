{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9wADwK78DCz"
      },
      "source": [
        "# Analytical Data Project: [Shopping Card Database]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eE0raob58DC0"
      },
      "source": [
        "## Determine Business Problems(QUESTIONS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmQeQ5YF8DC0"
      },
      "source": [
        "1. How has the company's sales and revenue performed in recent months?\n",
        "2. What are the most and least sold products?\n",
        "3. What are our customer demographics?\n",
        "4. When did the customer last make a transaction?\n",
        "5. How often has a customer made a purchase in the last few months?\n",
        "6. How much money did the customer spend in the last few months?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-z4QGlO8DC1"
      },
      "source": [
        "## Import Semua Packages/Library yang Digunakan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVYwaObI8DC1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_Sh51Xy8DC1"
      },
      "source": [
        "## Data Wrangling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXU2GBYu8DC1"
      },
      "source": [
        "### Gathering Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjCBk1BI8DC1"
      },
      "outputs": [],
      "source": [
        "# Loading the customers data\n",
        "customers_df = pd.read_csv(\"https://raw.githubusercontent.com/dicodingacademy/dicoding_dataset/main/DicodingCollection/customers.csv\")\n",
        "customers_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loading the orders data\n",
        "orders_df = pd.read_csv(\"https://raw.githubusercontent.com/dicodingacademy/dicoding_dataset/main/DicodingCollection/orders.csv\")\n",
        "orders_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loading the product data\n",
        "product_df = pd.read_csv(\"https://raw.githubusercontent.com/dicodingacademy/dicoding_dataset/main/DicodingCollection/products.csv\")\n",
        "product_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loading the sales data\n",
        "sales_df = pd.read_csv(\"https://raw.githubusercontent.com/dicodingacademy/dicoding_dataset/main/DicodingCollection/sales.csv\")\n",
        "sales_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMi6xGaDkbCi"
      },
      "source": [
        "**Insight:**\n",
        "- Okay, now we have successfully loaded all the required data. The next stage is to assess the quality of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHSiqaZp8DC1"
      },
      "source": [
        "### Assessing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ax-3tEjc9Cj1"
      },
      "outputs": [],
      "source": [
        "customers_df.info()\n",
        "customers_df.isna().sum()\n",
        "print(\"Jumlah duplikasi: \", customers_df.duplicated().sum())    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "customers_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "orders_df.info()\n",
        "print(\"Jumlah duplikasi: \",orders_df.duplicated().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "orders_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "product_df.info()\n",
        "print(\"Jumlah duplikasi: \", product_df.duplicated().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "product_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sales_df.info()\n",
        "sales_df.isna().sum()\n",
        "print(\"Jumlah duplikasi: \", sales_df.duplicated().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sales_df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dtxhAPrkhPL"
      },
      "source": [
        "**Insight:**\n",
        "- **costumers_df** = If you pay attention, there is a strangeness in the maximum value contained in the age column. This is most likely due to the presence of an inaccurate value in the column. We will also clean up this problem in the data cleaning stage.\n",
        "- **orders_df** = If you pay attention, there is no strangeness in the results. This shows that there is no duplication and strangeness of values in orders_df.\n",
        "- **product_df** = Based on the image above, it can be seen that there are 6 duplicated data in product_df. In the data cleaning stage, we will remove the duplication.\n",
        "- **sales_df** = The above results show that there is no duplication in the sales_df. In addition, it also shows that there is no strangeness in the summary of statistical parameters from sales_df."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhN5R4hr8DC1"
      },
      "source": [
        "### Cleaning Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**CUSTOMERS_DF**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVnYpprE9Evz"
      },
      "outputs": [],
      "source": [
        "### CLEANING customers_df Data\n",
        "\n",
        "# Based on the results of the data assessment process, it is known that there are three problems\n",
        "# encountered in the customer_df, namely duplicate data, missing value, and inaccurate value. At\n",
        "# this stage, we will clear up all three problems.\n",
        "\n",
        "# Eliminating duplicate data\n",
        "\n",
        "# The first problem we will deal with is duplicate data. As we have learned before, when we find\n",
        "# duplicates in the data, we must eliminate or delete those duplicates. Well, to do this, we can\n",
        "# make use of the drop_duplicates() method. Here is the code to remove duplicates on customer_df.\n",
        "\n",
        "customers_df.drop_duplicates(inplace=True)\n",
        "\n",
        "# After running the above code, double-check that there are still duplicates in the data by \n",
        "# running the following code.\n",
        "\n",
        "print(\"Jumlah duplikasi: \", customers_df.duplicated().sum())\n",
        "\n",
        "# If the deduplication process goes smoothly, the above code will produce an output indicating \n",
        "# the absence of duplicates on the customers_df.\n",
        "\n",
        "# Dealing with missing value\n",
        "\n",
        "# The next problem we have to deal with is the missing value in the gender column. Well, in \n",
        "# general, there are three methods to overcome missing value, namely dropping, imputation, and \n",
        "# interpolation. To determine which method to use, we need to look at the data that contains the\n",
        "# missing value using the following filtering technique.\n",
        "\n",
        "customers_df[customers_df.gender.isna()]\n",
        "\n",
        "# The above code will only display rows of data that meet the condition customers_df.gender.isna()\n",
        "# or in other words it will display rows of data that contain missing values in the gender column.\n",
        "# Here's what the row of data looks like.\n",
        "\n",
        "# Based on the image above, it can be seen that the data row still contains a lot of important\n",
        "# information so it would be a pity if it was thrown away immediately. Therefore, in this case,\n",
        "# we will use the imputation method to handle the missing value.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# In the imputation method, we will use a specific value to replace the missing value. The gender\n",
        "# column is a categorical column, we will use the dominant value as a substitute for the missing\n",
        "# value. Use the value_counts() method to identify the dominant value.\n",
        "\n",
        "customers_df.gender.value_counts()\n",
        "\n",
        "# The above code will produce the following output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Based on the results above, it can be seen that the most dominant value in the gender column is\n",
        "# \"Prefer not to say\". This value is what we will use next as a substitute for missing value.\n",
        "# This replacement process can be done using the fillna() method as in the following example.\n",
        "\n",
        "customers_df.fillna(value=\"Prefer not to say\", inplace=True)\n",
        "\n",
        "# To make sure the above process is running properly, we can re-run the code to identify the\n",
        "# missing value as follows.\n",
        "\n",
        "customers_df.isna().sum()\n",
        "\n",
        "# If the missing value cleanup process is successful, you will get the following results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Handling the innacurate value\n",
        "\n",
        "# Okay, now we're going to solve the problem of inaccurate values in the age column. For starters,\n",
        "# we need to look at the data row data that contains the inaccurate value (the row with the \n",
        "# maximum age value). This is done using a filter technique like the following code example.\n",
        "\n",
        "customers_df[customers_df.age == customers_df.age.max()]\n",
        "\n",
        "# The code above will display the rows of data that have the maximum age value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Based on this data, we can assume that the inaccurate value occurred due to human error so that\n",
        "# the excess entered a zero value. Therefore, replace it with a value of 70. This process is done\n",
        "# by utilizing the replace() method as shown in the following example.\n",
        "\n",
        "customers_df.age.replace(customers_df.age.max(), 70, inplace=True)\n",
        "\n",
        "# Well, to make sure the code above runs as expected, run the following code again.\n",
        "\n",
        "customers_df[customers_df.age == customers_df.age.max()]\n",
        "\n",
        "# Upsi, it turns out that there are still other invalid values contained in the age column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The cause of this error is likely to be the same as before, namely human error that is\n",
        "# overloaded with a zero value. To handle this, we'll replace it with a value of 50.\n",
        "\n",
        "customers_df.age.replace(customers_df.age.max(), 50, inplace=True)\n",
        "\n",
        "# To make sure there are no inaccurate values in the customers_df, run the following code.\n",
        "\n",
        "customers_df.describe()\n",
        "\n",
        "# The above code will produce the following output!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**ORDERS_DF**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CLEANING orders_df Data\n",
        "\n",
        "# Okay, now we have solved all the problems that exist in customers_df. Next, we will overcome\n",
        "# the problem in orders_df. Based on the previous data assessment process, it is known that there\n",
        "# is a data type error for the order_date & delivery_date columns. To solve this problem, we'll\n",
        "# replace the data type in the order_date & delivery_date columns to datetime. This process can \n",
        "# be done using  the to_datetime() function provided by the pandas library. Here's an example\n",
        "# code to do so.\n",
        "\n",
        "datetime_columns = [\"order_date\", \"delivery_date\"]\n",
        " \n",
        "for column in datetime_columns:\n",
        "  orders_df[column] = pd.to_datetime(orders_df[column])\n",
        "\n",
        "# The above code will change the data type in the order_date & delivery_date columns to datetime.\n",
        "# To make sure this works as expected, double-check the data type using the info() method.\n",
        "\n",
        "orders_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**PRODUCT_DF**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CLEANING product_df Data\n",
        "\n",
        "# The next data we will clean up is product_df. According to the results of the previous data\n",
        "# assessment, we know that there are 6 duplicate data in product_df. To solve this, we need to\n",
        "# discard the same data using the drop_duplicates() method as in the following example.\n",
        "\n",
        "product_df.drop_duplicates(inplace=True)\n",
        "\n",
        "# The code above will delete all duplicate data. To make sure the code works as expected, run the following code.\n",
        "\n",
        "print(\"Jumlah duplikasi: \", product_df.duplicated().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**SALES_DF**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CLEANING sales_df Data\n",
        "\n",
        "# The next data you need to clean up is sales_df. Based on the results of the previous data\n",
        "# analysis, it is known that there are 19 missing values in the total_price column. To find\n",
        "# out the most appropriate process for handling missing values, we need to first look at the\n",
        "# data rows that contain the missing values.\n",
        "\n",
        "sales_df[sales_df.total_price.isna()]\n",
        "\n",
        "# The code above will display all the rows of data that have missing values in the total_price\n",
        "# column as shown in the following image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Based on the display of the data, we find that the value of total_price is the result of\n",
        "# multiplication between price_per_unit and quantity. We can use this pattern to handle missing\n",
        "# values in total_price columns. Here's an example of implementing code to do this.\n",
        "\n",
        "sales_df[\"total_price\"] = sales_df[\"price_per_unit\"] * sales_df[\"quantity\"]\n",
        "\n",
        "# The code above will address all missing values and ensure that the values in total_price\n",
        "# columns are correct. To make sure of this, you can double-check the number of missing values\n",
        "# on the sales_df using the following code.\n",
        "\n",
        "sales_df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_5ejIqckiSP"
      },
      "source": [
        "**Insight:**\n",
        "- **customers_df** = Based on these results, it can be seen that the age column has a maximum value that is quite reasonable. In addition, if you pay attention, the mean and standard deviation values also change after we deal with the inaccurate value.\n",
        "- **orders_df** = If all stages go as expected, the above code will produce the following output.\n",
        "- **product_df** = If the process of deleting duplicate data goes smoothly, the above code will produce an output like the following \"Number of duplicates: 0\".\n",
        "- **sales_df** = If the previous process went smoothly, you will find the following results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MW7WF2kr8DC1"
      },
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**customers_df Data Exploration**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9CQCZjk8DC2"
      },
      "outputs": [],
      "source": [
        "#First, we'll explore customers_df data first. As we know, this dataset contains various\n",
        "# information related to customers, such as customer_id, customer_name, gender, age, home_address,\n",
        "# zip_code, city, state, and country.\n",
        "\n",
        "# For starters, we'll look at a summary of the statistical parameters of customers_df data using\n",
        "# the describe() method.\n",
        "\n",
        "customers_df.describe(include=\"all\")\n",
        "\n",
        "# The following is a summary of the statistical parameters obtained from the code above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Based on the summary of the statistical parameters above, we will obtain information on the\n",
        "# number of customers as many as 1001 people who are in the range between 20 to 80 years old with\n",
        "# an average age of 49.87 years with a standard deviation of 17.64 years. This information can\n",
        "# certainly give an idea that the customer segmentation we have is quite wide, ranging from\n",
        "# teenagers to the elderly.\n",
        "\n",
        "# Now let's take a look at customer demographics by gender. To do this, we'll use the groupby()\n",
        "# method followed by the agg() method. Here's an example code to do so.\n",
        "\n",
        "customers_df.groupby(by=\"gender\").agg({\n",
        "    \"customer_id\": \"nunique\",\n",
        "    \"age\": [\"max\", \"min\", \"mean\", \"std\"]\n",
        "})\n",
        "\n",
        "# The code above will generate a pivot table. It contains information on the number of customers\n",
        "# (unique from customer_id) as well as age parameters grouped by gender. The image below is a\n",
        "# view of the pivot table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Based on the pivot table above, it can be seen that the customers we have are dominated by\n",
        "# gender prefer not to say. On the other hand, their age distribution turned out to be quite\n",
        "# similar, ranging from 20 to 80 years.\n",
        "\n",
        "# Next, we try to see the distribution of the number of customers by city and state. To do this\n",
        "# we will also use the groupby() method. Also, to make the results easier to see, we'll sort\n",
        "# the values using the sort_values() method in a descending manner. Here's an example code to\n",
        "# do so.\n",
        "\n",
        "customers_df.groupby(by=\"city\").customer_id.nunique().sort_values(ascending=False)\n",
        "customers_df.groupby(by=\"state\").customer_id.nunique().sort_values(ascending=False)\n",
        "\n",
        "# The two codes will generate a pivot table as follows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Orders_df Data Exploration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The second data that we will explore is orders_df. It contains various information related\n",
        "# to an order consisting of order_id, customer_id, order_date, and delivery_date. Based on\n",
        "# this information, we can create a new column to contain the delivery time of each order.\n",
        "# To do this, we need to calculate the difference between the delivery_date and order_date and\n",
        "# store it as a delivery_time. Next, we'll use the apply() method to perform an operation on \n",
        "# each element in a DataFrame or Series column (a one-dimensional form of the DataFrame).\n",
        "# The operation we will do is to calculate the number of seconds of delivery_time using the\n",
        "# total_seconds() method. The value is then converted into a unit of days (divided by 86400) \n",
        "# and taken as an integer only. Here's an example code to do all of these processes.\n",
        "\n",
        "delivery_time = orders_df[\"delivery_date\"] - orders_df[\"order_date\"]\n",
        "delivery_time = delivery_time.apply(lambda x: x.total_seconds())\n",
        "orders_df[\"delivery_time\"] = round(delivery_time/86400)\n",
        "\n",
        "# After running the above code, you will find a new column on the DataFrame orders_df as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# To get a summary of statistical parameters from orders_df data, we can use the method\n",
        "# describe() as shown in the following code example.\n",
        "\n",
        "orders_df.describe(include=\"all\")\n",
        "\n",
        "# The code will produce a summary display of the following statistical parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Data Exploration orders_df and customers_df**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If you notice, in the orders_df data there is a column containing information related to\n",
        "# the customer id of the customer who has placed an order. We can use this information to\n",
        "# identify customers who have never placed an order. To do this, we can create a new column\n",
        "# named \"status\" in the customers_df data. The column has an \"Active\" value for customers\n",
        "# who have placed an order at least once and vice versa with a value of \"Non Active\" for\n",
        "# customers who have never placed an order at all. Here's the code to do so.\n",
        "\n",
        "customer_id_in_orders_df =  orders_df.customer_id.tolist()\n",
        "customers_df[\"status\"] = customers_df[\"customer_id\"].apply(lambda x: \"Active\" if x in customer_id_in_orders_df else \"Non Active\")\n",
        "customers_df.sample(5)\n",
        "\n",
        "# The code above will produce five sample data as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# To obtain information regarding the number of customers with \"Active\" and \"Non Active\"\n",
        "# status, we can use pivot tables. Here's an example code to create a pivot table using the\n",
        "# \"status\" column.\n",
        "\n",
        "customers_df.groupby(by=\"status\").customer_id.count()\n",
        "\n",
        "# The view of the pivot table will look like the one below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# As seen in the pivot table above, there are quite a lot of customers who have never made\n",
        "# a transaction before. This is certainly bad news for us because almost 30% of our customers\n",
        "# have never placed an order before.\n",
        "\n",
        "# To obtain more information related to these two data, we need to combine them through a \n",
        "# join or merge process. Here's an example of code for merging orders_df and customers_df data.\n",
        "\n",
        "orders_customers_df = pd.merge(\n",
        "    left=orders_df,\n",
        "    right=customers_df,\n",
        "    how=\"left\",\n",
        "    left_on=\"customer_id\",\n",
        "    right_on=\"customer_id\"\n",
        ")\n",
        "orders_customers_df.head()\n",
        "\n",
        "# In the code above, we perform the merge process with the \"left\" method. Of course, you\n",
        "# still remember not with this method. Yep, it's true that this method allows us to retrieve\n",
        "# all the values from the left table as well as the values that correspond to the right table.\n",
        "# Here's what the data looks like from the code above.\n",
        "\n",
        "# There is a lot of information that we can explore from the data above. Here are some things\n",
        "# to explore."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**1. Number of orders by city**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We can create a pivot table to obtain information related to the number of orders by city\n",
        "# with the following code. The code above, will produce a pivot table view as shown below.\n",
        "\n",
        "orders_customers_df.groupby(by=\"city\").order_id.nunique().sort_values(ascending=False).reset_index().head(10)\n",
        "\n",
        "# Well, based on these results, it is known that the cities of Jordanside and New Ava are the\n",
        "# two cities that have the highest number of orders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**2. Number of orders by state**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now we will look at the number of orders by state. To obtain information related to this, we\n",
        "# can also use pivot tables. Use the code below to implement it. The code will generate a pivot\n",
        "# table as shown below. Based on the pivot table, it is known that South Australia is a state\n",
        "# that makes a lot of orders.\n",
        "\n",
        "orders_customers_df.groupby(by=\"state\").order_id.nunique().sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**3. Number of orders by gender**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The next information that we can explore is the number of orders by gender. To do this, of\n",
        "# course, we need to create a pivot table using the following code. The code above will produce\n",
        "# a pivot table view as follows.\n",
        "\n",
        "orders_customers_df.groupby(by=\"gender\").order_id.nunique().sort_values(ascending=False)\n",
        "\n",
        "# Based on these results, it can be seen that most orders are made by customers who prefer not\n",
        "# to say. This is certainly in line with the number of customers, most of whom are from this \n",
        "# gender group.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**4. Number of orders by age group**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Last but not least, we can also explore the number of orders by age group. To do this, we need\n",
        "# to define a new column named \"age_group\". This column will help us in grouping customers into\n",
        "# three groups, namely youth, adults, and seniors. Next, create a pivot table based on this\n",
        "# using the example code below. The code above, will produce a pivot table view as follows.\n",
        "\n",
        "orders_customers_df[\"age_group\"] = orders_customers_df.age.apply(lambda x: \"Youth\" if x <= 24 else (\"Seniors\" if x > 64 else \"Adults\"))\n",
        "orders_customers_df.groupby(by=\"age_group\").order_id.nunique().sort_values(ascending=False)\n",
        "\n",
        "# Based on these results, it is known that the customers who make the most orders come from\n",
        "# the Adults age group.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Data Exploration product_df and sales_df**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# At this stage, we will explore product_df data and sales_df. For starters, we can see a \n",
        "# summary of the statistical parameters of the two using the describe() method.\n",
        "\n",
        "product_df.describe(include=\"all\")\n",
        "sales_df.describe(include=\"all\")\n",
        "\n",
        "# The two codes will each produce a summary of statistical parameters as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Based on these results, it can be seen that the price of the goods sold ranges from 90 to 119\n",
        "# dollars. In addition, we also get other information that is no less interesting, namely in\n",
        "# each transaction, customers buy a maximum of three items in one type of product with a total\n",
        "# price of 357 dollars.\n",
        "\n",
        "# If you are curious about the product that has the most expensive and lowest price, please run\n",
        "# the following code to see it.\n",
        "\n",
        "product_df.sort_values(by=\"price\", ascending=False)\n",
        "\n",
        "# Here's what the results of the code above look like. It can be seen that the most expensive\n",
        "# product is a jacket item called Parka and the cheapest one is called Bomber."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Next, we can use pivot tables to search for information related to products based on the type\n",
        "# and name of the product. Here's an example of code you can use.\n",
        "\n",
        "product_df.groupby(by=\"product_type\").agg({\n",
        "    \"product_id\": \"nunique\",\n",
        "    \"quantity\": \"sum\",\n",
        "    \"price\":  [\"min\", \"max\"]\n",
        "})\n",
        " \n",
        "product_df.groupby(by=\"product_name\").agg({\n",
        "    \"product_id\": \"nunique\",\n",
        "    \"quantity\": \"sum\",\n",
        "    \"price\": [\"min\", \"max\"]\n",
        "})\n",
        "\n",
        "# The two codes will each generate a pivot table as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The pivot table above can give us an overview of the products sold by the Dicoding Collection.\n",
        "# As a prospective reliable data practitioner, of course you are curious about the best-selling\n",
        "# products. Well, to answer this question, we need to merge the product_df and sales_df tables\n",
        "# with the following code.\n",
        "\n",
        "sales_product_df = pd.merge(\n",
        "    left=sales_df,\n",
        "    right=product_df,\n",
        "    how=\"left\",\n",
        "    left_on=\"product_id\",\n",
        "    right_on=\"product_id\"\n",
        ")\n",
        "sales_product_df.head()\n",
        "\n",
        "# Basically, the code above will perform a merge process on product_df and sales_df data. \n",
        "# The merge process is carried out using the \"left\" method. The following are the results of the\n",
        "# merge process.\n",
        "\n",
        "# If you look back, the result of the merger process above has a difference between the\n",
        "# price_per_unit and price values. This can happen due to discounts, operational costs, \n",
        "# and other costs.\n",
        "\n",
        "# Okay, now let's try to look at the product sales information by type. Surely you can already\n",
        "# guess what technique we will use to obtain this information, right? Yep, that's right, \n",
        "# we're going to create a pivot table based on product type with the following code.\n",
        "\n",
        "sales_product_df.groupby(by=\"product_type\").agg({\n",
        "    \"sales_id\": \"nunique\",\n",
        "    \"quantity_x\": \"sum\",\n",
        "    \"total_price\": \"sum\"\n",
        "})\n",
        "\n",
        "# Here's the pivot table obtained from the code above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If you pay attention to the pivot table above, Trousers are the best-selling product type.\n",
        "# However, when viewed based on the revenue received, Jacket is the type of product that\n",
        "# contributes the most to the company's revenue.\n",
        "\n",
        "# We can create the same pivot table to view sales information based on product name using the\n",
        "# code below\n",
        "\n",
        "sales_product_df.groupby(by=\"product_name\").agg({\n",
        "    \"sales_id\": \"nunique\",\n",
        "    \"quantity_x\": \"sum\",\n",
        "    \"total_price\": \"sum\"\n",
        "}).sort_values(by=\"total_price\", ascending=False)\n",
        "\n",
        "# The code above will generate a pivot table as follows.\n",
        "\n",
        "# Based on the pivot table, it is known that Denim products are the best-selling products and\n",
        "# also contribute the most revenue to the company."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**all_df Data Exploration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# if you pay attention to this sales data is quite interesting to explore deeper. This is done\n",
        "# to see purchase patterns based on customer demographics. Therefore, we need to create a new\n",
        "# DataFrame named all_df to hold all the information from the four tables we have. Here's an\n",
        "# example of code that we can use to run this merge process.\n",
        "\n",
        "all_df = pd.merge(\n",
        "    left=sales_product_df,\n",
        "    right=orders_customers_df,\n",
        "    how=\"left\",\n",
        "    left_on=\"order_id\",\n",
        "    right_on=\"order_id\"\n",
        ")\n",
        "all_df.head()\n",
        "\n",
        "# The merge process above will produce a DataFrame as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Next, let's try to look at purchasing preferences based on customer state and product type\n",
        "# using the code below.\n",
        "\n",
        "ll_df.groupby(by=[\"state\", \"product_type\"]).agg({\n",
        "    \"quantity_x\": \"sum\",\n",
        "    \"total_price\": \"sum\"\n",
        "})\n",
        "\n",
        "# The code above will generate a pivot table as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The pivot table above gives us an idea of the types of products that users prefer based on\n",
        "# their state location. If you pay attention, the Jacket product type (red line) is most sold\n",
        "# in the states of Queensland, South Australia, and New South Wales. For other product types,\n",
        "# you can see it in the pivot table image above.\n",
        "\n",
        "# Now you can do the same to find out the taste of customers' product types based on gender and\n",
        "# age group. Here's an example of code you can use.\n",
        "\n",
        "all_df.groupby(by=[\"gender\", \"product_type\"]).agg({\n",
        "    \"quantity_x\": \"sum\",\n",
        "    \"total_price\": \"sum\"\n",
        "})\n",
        " \n",
        "all_df.groupby(by=[\"age_group\", \"product_type\"]).agg({\n",
        "    \"quantity_x\": \"sum\",\n",
        "    \"total_price\": \"sum\"\n",
        "})\n",
        "\n",
        "# The two codes above will generate each of the following pivot tables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The pivot table above can give us an idea of the taste of customer product types based\n",
        "# on gender and age group. All of this information will be of great help to you in answering\n",
        "# questions related to user preferences based on their demographics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th_Lzl2Fkj9O"
      },
      "source": [
        "**Insight:**\n",
        "1. customer_df = Based on the results above, it can be seen that the distribution of our customers is quite evenly distributed in every city and state. Our customers are most in the cities of East Aidan, East Sophia, and New Ava with three customers each. In addition, most of our customers are from the state of South Australia.\n",
        "2. orders_df = Based on these results, it can be seen that the average delivery time is 14 days with a maximum value of 27 days and a minimum value of 1 day.\n",
        "3. orders_df and customers_df = Read the code above\n",
        "4. product_df and sales_df = read the code above\n",
        "5. all_df = read the code above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsyZjqak8DC2"
      },
      "source": [
        "## Visualization & Explanatory Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZxOiQ6n8DC2"
      },
      "source": [
        "### Pertanyaan 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1swJUdAD8DC2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgHI7CiU8DC2"
      },
      "source": [
        "### Pertanyaan 2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Go0lCsvO8DC2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0-36BDLklRg"
      },
      "source": [
        "**Insight:**\n",
        "- xxx\n",
        "- xxx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9y4VUsmcYNZ5"
      },
      "source": [
        "## Analisis Lanjutan (Opsional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWhnzsJGYUCO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WeHlCeX8DC2"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTcyR48Y8DC2"
      },
      "source": [
        "- Conclution pertanyaan 1\n",
        "- Conclution pertanyaan 2"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0 (tags/v3.8.0:fa919fd, Oct 14 2019, 19:21:23) [MSC v.1916 32 bit (Intel)]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "972b3bf27e332e87b5379f2791f6ef9dfc79c71018c370b0d7423235e20fe4d7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
